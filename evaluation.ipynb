{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a7b14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d83789bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "711b912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b74cc5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing details\n",
    "data_transforms = {\n",
    "\t'train': transforms.Compose([\n",
    "\t\ttransforms.RandomResizedCrop(224),\n",
    "\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t]),\n",
    "\t'val': transforms.Compose([\n",
    "\t\ttransforms.Resize(224),\n",
    "\t\ttransforms.CenterCrop(224),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6543a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"/home/rubel/projects_works/projects/algorithms/open_set_recognition/dataset/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf85ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = \"/home/rubel/projects_works/projects/algorithms/open_set_recognition/weights/model_best.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5e84dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "6358\n"
     ]
    }
   ],
   "source": [
    "class_list = [os.path.basename(i) for i in glob.glob(os.path.join(src_path, \"*\"))]\n",
    "class_list.remove(\"unknown_class\")\n",
    "class_list.sort()\n",
    "print(len(class_list))\n",
    "img_list = []\n",
    "for c in class_list:\n",
    "    img_names = glob.glob(os.path.join(src_path, c , \"*\"))\n",
    "    for img in img_names:\n",
    "        img_list.append([img, c])\n",
    "img_names = glob.glob(os.path.join(src_path, \"unknown_class\", \"*\"))\n",
    "for img in img_names:\n",
    "    img_list.append([img, \"unknown_class\"])\n",
    "random.shuffle(img_list)\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b847e723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dettol_asl_120ml',\n",
       " 'dettol_asl_235ml',\n",
       " 'dettol_asl_725ml',\n",
       " 'dettol_mac_aqua_curved_650ml',\n",
       " 'dettol_mac_jasmine_curved_650ml',\n",
       " 'dettol_mac_lavander_curved_650ml',\n",
       " 'dettol_mac_lemon_curved_650ml',\n",
       " 'dettol_spray_crisp_breeze_450ml',\n",
       " 'dettol_spray_morning_dew_450ml']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f80d25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(weight_path):\n",
    "    # loading the trained model and generating embedding based on that\n",
    "    base_model = models.resnet18(pretrained=False).to(DEVICE)\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = base_model.fc.in_features\n",
    "    base_model.fc = nn.Sequential(nn.Linear(num_ftrs, 256), nn.Linear(256, 128))\n",
    "    base_model = base_model.to(DEVICE)\n",
    "\n",
    "    # loading the trained model with trained weights\n",
    "    checkpoint = torch.load(weight_path)\n",
    "    base_model.load_state_dict(checkpoint['state_dict'])\n",
    "    base_model = base_model.eval()\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80f18595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the pretrained model and changing the dense layer. Initially the convolution layers will be freezed\n",
    "base_model = models.resnet18(pretrained=True).to(DEVICE)\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = base_model.fc.in_features\n",
    "base_model.fc = nn.Sequential(nn.Linear(num_ftrs, 256), nn.Linear(256, 128), nn.Linear(128, len(class_list)), nn.Softmax(dim=1))\n",
    "base_model = base_model.to(DEVICE)\n",
    "\n",
    "\n",
    "# registering a forward hook to extract features\n",
    "feature = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        feature[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "base_model.fc[1].register_forward_hook(get_activation(\"embeddings\"))\n",
    "\n",
    "checkpoint = torch.load(weight_path)\n",
    "base_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "base_model = base_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c638cc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6358it [01:47, 59.15it/s]\n"
     ]
    }
   ],
   "source": [
    "total_unknown = 0\n",
    "correct_unknown = 0\n",
    "false_unknown = 0\n",
    "\n",
    "total_known = 0\n",
    "correct_known = 0\n",
    "false_known = 0\n",
    "for idx, i in tqdm(enumerate(img_list)):\n",
    "    img_name = i[0]\n",
    "    label = i[1]\n",
    "    query_img = Image.open(img_name)\n",
    "    query_img = data_transforms[\"val\"](query_img)\n",
    "    query_img = query_img.unsqueeze(0).to(DEVICE)\n",
    "    query_img_output = base_model(query_img)[0]\n",
    "    embeddings = feature[\"embeddings\"]\n",
    "    embeddings_sum = torch.sqrt(torch.sum(torch.square(embeddings), axis=1)).cpu()\n",
    "    if label == \"unknown_class\":\n",
    "        total_unknown += 1\n",
    "    else:\n",
    "        total_known += 1\n",
    "    if embeddings_sum.item() < 5.5:\n",
    "        if label == \"unknown_class\":\n",
    "            correct_unknown += 1\n",
    "        else:\n",
    "            false_known += 1\n",
    "    elif embeddings_sum.item() > 5.5:\n",
    "        if label == \"unknown_class\":\n",
    "            false_unknown += 1\n",
    "        else:\n",
    "            correct_known += 1\n",
    "#     print(torch.max(query_img_output).item(), label, embeddings_sum.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2108423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1396 604\n",
      "4358 4059 299\n"
     ]
    }
   ],
   "source": [
    "print(total_unknown, correct_unknown, false_unknown)\n",
    "print(total_known, correct_known, false_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "217865b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1049 951\n",
      "4358 3046 1312\n"
     ]
    }
   ],
   "source": [
    "print(total_unknown, correct_unknown, false_unknown)\n",
    "print(total_known, correct_known, false_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bc1eb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 777 1223\n",
      "4358 4336 22\n"
     ]
    }
   ],
   "source": [
    "print(total_unknown, correct_unknown, false_unknown)\n",
    "print(total_known, correct_known, false_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd518065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tprint(\"Staring accuracy check on test data...\")\n",
    "\tfor idx, i in tqdm(enumerate(test_img_list)):\n",
    "\t\tquery_img_org = Image.open(i)\n",
    "\t\tgt_class = i.split(\"/\")[-2]\n",
    "\t\tquery_img = config.data_transforms[\"val\"](query_img_org)\n",
    "\t\tquery_img = query_img.unsqueeze(0).to(config.DEVICE)\n",
    "\t\tquery_img_embedding = base_model(query_img)\n",
    "\t\tquery_img_embedding = query_img_embedding.squeeze()\n",
    "\n",
    "\t\tsimilar_images = annoy_index.get_nns_by_vector(query_img_embedding, 20, include_distances=True)\n",
    "\t\tsimilar_image_labels = [annoy_index_to_label[i] for i in similar_images[0]]\n",
    "\t\t\n",
    "\t\tpt_class = similar_image_labels[0]\n",
    "\t\t\n",
    "\t\tif gt_class == pt_class:\n",
    "\t\t\tif gt_class in individual_accuracy:\n",
    "\t\t\t\tindividual_accuracy[gt_class][0] += 1\n",
    "\t\t\t\tindividual_accuracy[gt_class][1] += 1\n",
    "\t\t\t\tindividual_accuracy[gt_class][2] = individual_accuracy[gt_class][1]/individual_accuracy[gt_class][0]\n",
    "\t\t\telse:\n",
    "\t\t\t\tindividual_accuracy[gt_class] = [1, 1, 1]\n",
    "\t\t\t\t\n",
    "\t\t\tcorrect += 1\n",
    "\t#         query_img_org.save(\"./wrong_predictions/test/correct/\" + os.path.splitext(os.path.basename(i))[0] + \"_gt_\" + gt_class + \"_pt_\" + pt_class + \".png\")\n",
    "\t\telse:\n",
    "\t\t\tif gt_class in individual_accuracy:\n",
    "\t\t\t\tindividual_accuracy[gt_class][0] += 1\n",
    "\t\t\t\tindividual_accuracy[gt_class][1] += 0\n",
    "\t\t\t\tindividual_accuracy[gt_class][2] = individual_accuracy[gt_class][1]/individual_accuracy[gt_class][0]\n",
    "\t\t\telse:\n",
    "\t\t\t\tindividual_accuracy[gt_class] = [1, 0, 0]\n",
    "\t#         query_img_org.save(\"./wrong_predictions/test/wrong/\" + os.path.splitext(os.path.basename(i))[0] + \"_gt_\" + gt_class + \"_pt_\" + pt_class + \".png\")\n",
    "\t\ttotal += 1\n",
    "\t\n",
    "\tprint(\"Overall Accuracy \", correct/total)\n",
    "\tprint(\"Individual Accuracy Report:\")\n",
    "\tprint(individual_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
